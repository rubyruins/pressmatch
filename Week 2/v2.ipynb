{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import Algorithmia\n",
    "import bs4 as bs\n",
    "import lxml\n",
    "from urllib.request import Request, urlopen\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news.pkl', 'rb') as f:\n",
    "    news = pickle.load(f)\n",
    "    # 3863 unique authors\n",
    "\n",
    "muckrack = pd.read_csv('muckrack_persons_fetchlist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Algorithmia.algorithm.Algorithm at 0x1fbf6c9ca48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Algorithmia.client('simKLMHlWDMzWd+QcNRqVhI104r1')\n",
    "algo = client.algo('specrom/Google_scraper/0.1.4')\n",
    "algo.set_options(timeout=300) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_top(n):\n",
    "    temp = news[['author', 'article_count']].groupby('author').count().sort_values(['article_count'], ascending = False)\n",
    "    temp = temp.reset_index().head(n)\n",
    "    urls = []\n",
    "    for i in range(len(temp)):\n",
    "        url = temp.loc[i].author.lower()\n",
    "        url = re.sub(r'\\|.*$', '', url) # remove everything after |\n",
    "        url = re.sub(r'\\,.*$', '', url) # remove evrything after ,\n",
    "        \n",
    "        punctuations = '''!()[]{};:'\"\\,<>./?#$%^&*_~'''\n",
    "        for x in url.lower(): \n",
    "            if x in punctuations: url = url.replace(x, \"\") # remove punctuations except hyphen\n",
    "        url = re.sub(r'(\\d+)', '', url) # remove numbers\n",
    "        url = re.sub(r'(--)', '', url)\n",
    "        url = ' '.join([i.strip() for i in url.split()]) # remove spaces and lowercase\n",
    "        url = re.sub(r'(\\w*@\\w*)', '', url) # remove emails\n",
    "        \n",
    "        url = re.sub(r'(sa)(\\s+)(\\w+)(\\s+)', '', url) # SA editors\n",
    "        url = re.sub(r'(tulsa)(\\s+)(world)', '', url) # tulsa world editors\n",
    "        url = re.sub(r'(world-herald)(\\s*)(\\w*)(\\s*)(\\w*)', '', url) # world herald editors\n",
    "        url = re.sub(r'(news)(\\s+)(editor)', '', url) # news editors\n",
    "        url = re.sub(r'(richmond)(\\s+)(times-dispatch)', '', url) # richmond editors\n",
    "        url = re.sub(r'(new)(\\s+)(hampshire)(\\s+)(union)(\\s+)(leader)', '', url) # new hampshire editors\n",
    "        url = re.sub(r'(for)(\\s+)(the)(\\s+)(state)(\\s+)(journal)', '', url) # state journal editors\n",
    "        url = re.sub(r'(union)(\\s+)(leader)(\\s+)(correspondent)', '', url) # union leader correspondants\n",
    "        url = re.sub(r'(sfgate)', '', url) # sfgate correspondants\n",
    "        url = re.sub(r'(special)(\\s+)(from)(\\s+)(the)(\\s+)(gazette)', '', url) # the gazette correspondants\n",
    "        url = re.sub(r'(times)(\\s+)(correspondent)', '', url) # times correspondants\n",
    "        \n",
    "        url = 'https://muckrack.com/' + url.replace(' ', '-').strip('-') # make URL\n",
    "        urls.append(url)\n",
    "    temp['Muckrack'] = urls\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    info = {}\n",
    "    info['name'] = data.iloc[j].author\n",
    "    if soup.find_all(class_ = 'profile-verified'):\n",
    "        info['verified'] = True\n",
    "    if soup.find(class_ = 'person-details-item person-details-beats'):\n",
    "        info['interests'] = ' '.join([line.strip() for line in soup.find(class_ = 'fa fa-fw fa-tasks icon-standard').findNext('div').text.split('\\n') if line.strip() != ''])\n",
    "    if soup.find(class_ = 'person-details-item person-details-title'):\n",
    "        info['title'] = ' '.join([line.strip() for line in soup.find(class_ = 'fa fa-fw fa-building icon-standard').findNext('div').text.split('\\n') if line.strip() != ''])\n",
    "    if soup.find(class_ = 'person-details-item person-details-location'):\n",
    "        info['location'] = ' '.join([line.strip() for line in soup.find(class_ = 'fa fa-fw fa-map-marker icon-standard').findNext('div').text.split('\\n') if line.strip() != ''])\n",
    "    if soup.find(class_ = 'mr-font-size-lg mr-font-family-2 top-sm'):\n",
    "        info['description'] = soup.find(class_ = 'mr-font-size-lg mr-font-family-2 top-sm').text.strip()\n",
    "    info['seen_in'] = list(set([i.text for i in soup.find_all('a') if '/media-outlet/' in i.attrs['href']]))\n",
    "    if soup.find(class_ = 'profile-section-social'):\n",
    "        for i in soup.find(class_ = 'profile-section-social'):\n",
    "            temp = [j[5:] for j in str(i).split() if 'href' in j]\n",
    "            if temp: \n",
    "                temp = temp[0].strip('\"')\n",
    "                if 't.co' in temp:\n",
    "                    info['website'] = temp\n",
    "                if 'twitter' in temp:\n",
    "                    info['twitter'] = temp\n",
    "                if 'linkedin' in temp:\n",
    "                    info['linkedin'] = temp\n",
    "                if 'facebook' in temp:\n",
    "                    info['facebook'] = temp\n",
    "                if 'instagram' in temp:\n",
    "                    info['instagram'] = temp      \n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Neil Shaw', 'verified': True, 'interests': 'United Kingdom', 'title': 'Network Content Editor â€” DevonLive, Reach plc', 'location': 'England', 'description': 'Network Content Editor for Reach Plc. All of my words are second-hand, and useless in the face of this.', 'seen_in': ['The Washington Post', 'Reach plc', 'Al Jazeera', 'Daily Star (UK)', 'MSN South Africa', 'Fox News', 'Associated Press', 'DevonLive', 'Daily Mirror', 'The (Toronto) Star', 'MSN UK', 'Sunday Mirror'], 'website': 'https://t.co/dTqiiMyFdj', 'twitter': 'http://twitter.com/neiljshaw', 'linkedin': 'https://www.linkedin.com/in/neil-shaw-b45369a/'}\n",
      "\n",
      "{'name': 'James Rodger', 'verified': True, 'interests': 'United Kingdom', 'title': 'Head of Trends â€” Birmingham Live', 'location': 'Birmingham', 'description': 'Regional Content Editor for @birmingham_live.', 'seen_in': ['Wales on Sunday', 'Rochdale Observer', 'Daily Star (UK)', 'Western Mail', 'MSN South Africa', 'Daily Mirror', 'Birmingham Live', 'MSN UK', 'Sunday Mirror', 'Liverpool Echo', 'Sunday Mercury'], 'twitter': 'http://twitter.com/jamesdrodger', 'facebook': 'https://www.facebook.com/jamesrodgerjournalist', 'linkedin': 'https://www.linkedin.com/in/james-rodger-4325348a/'}\n",
      "\n",
      "{'name': 'Jack Davis', 'verified': True, 'interests': 'Politics, Religion', 'title': 'Freelance Writer â€” Freelance', 'location': 'New York', 'seen_in': ['Bozeman Daily Chronicle', 'The Western Journal', 'Freelance', 'Farms.com', 'Entrepreneur Magazine', 'Salt Lake Tribune', 'Bristol Post', 'AudioFile Magazine', 'Irish Times', 'News Medical', 'WND']}\n",
      "\n",
      "{'name': 'Adam Wells', 'verified': True, 'interests': 'Sports, U.S.', 'title': 'Writer â€” Bleacher Report', 'location': 'Boston', 'description': 'Kinda funny looking, just in a general sorta way. I am in Sports.', 'seen_in': ['Yahoo', 'Bleacher Report', 'JD Supra', 'True Median', 'Saints Report', 'NBA.com'], 'website': 'https://bleacherreport.com/users/340764', 'twitter': 'http://twitter.com/adamwells1985', 'linkedin': 'https://www.linkedin.com/in/adam-wells-07a756ab/'}\n",
      "\n",
      "{'name': 'Sophie McCoid', 'verified': True, 'interests': 'Arts and Entertainment, United Kingdom', 'title': 'Senior Reporter â€” Liverpool Echo', 'location': 'Liverpool', 'description': 'Senior Reporter for @LivEchonews, Covering TV, Showbiz and everything in between Got a story? sophie.mccoid@reachplc.com', 'seen_in': ['Wales on Sunday', 'Rochdale Observer', 'Daily Star (UK)', 'Western Mail', 'MSN South Africa', 'Daily Mirror', 'Birmingham Live', 'MSN UK', 'Sunday Mirror', 'Liverpool Echo', 'Sunday Mercury'], 'twitter': 'http://twitter.com/MccoidSophie'}\n",
      "\n",
      "{'name': 'Joseph Zucker', 'verified': True, 'interests': 'Sports, U.S.', 'title': 'Breaking News Writer â€” Bleacher Report', 'description': \"@bleacherreport breaking news writer. Ohio's biggest Seattle Storm fan\", 'seen_in': ['High Post Hoops', 'FanSided', 'RealClearLife', 'Bleacher Report', 'The Oklahoma Eagle', 'KZDC-AM (San Antonio, TX)', 'The Independent', 'The Charlotte Observer', 'Saints Report'], 'website': 'https://t.co/jGmeIFUX06', 'twitter': 'http://twitter.com/JosephZucker', 'linkedin': 'https://www.linkedin.com/in/joseph-zucker-49956693'}\n",
      "\n",
      "{'name': 'Jack Otway', 'verified': True, 'interests': 'Sports, United Kingdom', 'title': 'Freelance Journalist, Reporter â€” Daily Express, Screen Rant', 'location': 'England', 'description': 'Senior sport reporter for the Daily Express Online âš½\\nFreelance journalist for Screen Rant ðŸŽ¥\\nPrevious work: Daily Mail, GiveMeSport, WhatCulture, Daily Star', 'seen_in': ['Daily Star (UK)', 'Bleacher Report', 'WhatCulture', 'Screen Rant', 'Daily Express'], 'twitter': 'http://twitter.com/JackOtwayJourno', 'linkedin': 'https://www.linkedin.com/in/jack-otway-2422a0103', 'instagram': 'https://www.instagram.com/jack_otway123/?hl=en'}\n",
      "\n",
      "{'name': 'Simon Duke', 'verified': True, 'interests': 'Media, Technology, United Kingdom', 'title': 'Journalist â€” The Times', 'location': 'London', 'description': 'News, comment and features on technology and media for the Times. @TimesBusiness', 'seen_in': ['The Sunday Times', 'Wales on Sunday', 'Rochdale Observer', 'Western Mail', 'The Times', 'Daily Mirror', 'The Australian', 'Manchester Evening News', 'Birmingham Live', 'Sunday Mirror', 'Liverpool Echo'], 'website': 'https://t.co/GAsNZjVp0D', 'twitter': 'http://twitter.com/SimonDukeTimes', 'linkedin': 'https://www.linkedin.com/in/simon-duke-161245170/'}\n",
      "\n",
      "{'name': 'Maren Estrada', 'verified': True, 'interests': 'Technology', 'title': 'Contributor â€” BGR', 'seen_in': ['Yahoo', 'MSN Canada', 'Yahoo Tech', 'Decider', 'Epoch Times', 'MSN', 'Yahoo Singapore', 'BGR']}\n",
      "\n",
      "{'name': 'Dave Johnson', 'verified': True, 'interests': 'Canada', 'title': 'Reporter and Photographer â€” Welland Tribune', 'location': 'Lake Erie, Niagara', 'description': 'Reporter/photographer at The Welland Tribune (Torstar/Metroland Media)', 'seen_in': ['Yahoo', 'Common Dreams', 'Heavy.com', 'KHOU-TV (Houston, TX)', 'Salon', 'Medium', 'Out Magazine', 'The (Toronto) Star', 'Welland Tribune', 'Business Insider Singapore', 'Toronto Sun'], 'twitter': 'http://twitter.com/DaveJTheTrib', 'instagram': 'https://www.instagram.com/davejthetrib/?hl=en'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(data)):\n",
    "    try:\n",
    "        url = data.iloc[j].Muckrack\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        sauce = urlopen(req).read()\n",
    "    except:\n",
    "        print(\"wrong url\", j, data.iloc[j].author)\n",
    "        query = {\"query\": f\"{data.iloc[j].author} site: muckrack\"}\n",
    "        url = algo.pipe(query).result\n",
    "        url = url[0]['url']\n",
    "        if 'muckrack' in url:    \n",
    "            print(\"right url\", url)\n",
    "            print()\n",
    "            req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            sauce = urlopen(req).read()\n",
    "    soup = bs.BeautifulSoup(sauce,'lxml')\n",
    "    extract_data(soup)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
