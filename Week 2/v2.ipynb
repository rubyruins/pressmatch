{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import Algorithmia\n",
    "import bs4 as bs\n",
    "import lxml\n",
    "from urllib.request import Request, urlopen\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/news.pkl', 'rb') as f:\n",
    "    news = pickle.load(f)\n",
    "    # 3844 unique authors\n",
    "\n",
    "muckrack = pd.read_csv('../Data/muckrack_persons_fetchlist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Algorithmia.algorithm.Algorithm at 0x1fbf6c9ca48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Algorithmia.client('simKLMHlWDMzWd+QcNRqVhI104r1')\n",
    "algo = client.algo('specrom/Google_scraper/0.1.4')\n",
    "algo.set_options(timeout=300) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_top(n):\n",
    "    temp = news[['author', 'article_count']].groupby('author').count().sort_values(['article_count'], ascending = False)\n",
    "    temp = temp.reset_index().head(n)\n",
    "    urls = []\n",
    "    for i in range(len(temp)):\n",
    "        url = temp.loc[i].author.lower()\n",
    "        url = re.sub(r'\\|.*$', '', url) # remove everything after |\n",
    "        url = re.sub(r'\\,.*$', '', url) # remove evrything after ,\n",
    "        \n",
    "        punctuations = '''!()[]{};:'\"\\,<>./?#$%^&*_~'''\n",
    "        for x in url.lower(): \n",
    "            if x in punctuations: url = url.replace(x, \"\") # remove punctuations except hyphen\n",
    "        url = re.sub(r'(\\d+)', '', url) # remove numbers\n",
    "        url = re.sub(r'(--)', '', url)\n",
    "        url = ' '.join([i.strip() for i in url.split()]) # remove spaces and lowercase\n",
    "        url = re.sub(r'(\\w*@\\w*)', '', url) # remove emails\n",
    "        \n",
    "        url = re.sub(r'(sa)(\\s+)(\\w+)(\\s+)', '', url) # SA editors\n",
    "        url = re.sub(r'(tulsa)(\\s+)(world)', '', url) # tulsa world editors\n",
    "        url = re.sub(r'(world-herald)(\\s*)(\\w*)(\\s*)(\\w*)', '', url) # world herald editors\n",
    "        url = re.sub(r'(news)(\\s+)(editor)', '', url) # news editors\n",
    "        url = re.sub(r'(richmond)(\\s+)(times-dispatch)', '', url) # richmond editors\n",
    "        url = re.sub(r'(new)(\\s+)(hampshire)(\\s+)(union)(\\s+)(leader)', '', url) # new hampshire editors\n",
    "        url = re.sub(r'(for)(\\s+)(the)(\\s+)(state)(\\s+)(journal)', '', url) # state journal editors\n",
    "        url = re.sub(r'(union)(\\s+)(leader)(\\s+)(correspondent)', '', url) # union leader correspondants\n",
    "        url = re.sub(r'(sfgate)', '', url) # sfgate correspondants\n",
    "        url = re.sub(r'(special)(\\s+)(from)(\\s+)(the)(\\s+)(gazette)', '', url) # the gazette correspondants\n",
    "        url = re.sub(r'(times)(\\s+)(correspondent)', '', url) # times correspondants\n",
    "        \n",
    "        url = 'https://muckrack.com/' + url.replace(' ', '-').strip('-') # make URL\n",
    "        urls.append(url)\n",
    "    temp['Muckrack'] = urls\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_top(3844)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>article_count</th>\n",
       "      <th>Muckrack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neil Shaw</td>\n",
       "      <td>523</td>\n",
       "      <td>https://muckrack.com/neil-shaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>James Rodger</td>\n",
       "      <td>512</td>\n",
       "      <td>https://muckrack.com/james-rodger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jack Davis</td>\n",
       "      <td>251</td>\n",
       "      <td>https://muckrack.com/jack-davis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adam Wells</td>\n",
       "      <td>223</td>\n",
       "      <td>https://muckrack.com/adam-wells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sophie McCoid</td>\n",
       "      <td>218</td>\n",
       "      <td>https://muckrack.com/sophie-mccoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839</th>\n",
       "      <td>Meredith Cohn</td>\n",
       "      <td>10</td>\n",
       "      <td>https://muckrack.com/meredith-cohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3840</th>\n",
       "      <td>Kate Duguid</td>\n",
       "      <td>10</td>\n",
       "      <td>https://muckrack.com/kate-duguid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>Jonathan Dean</td>\n",
       "      <td>10</td>\n",
       "      <td>https://muckrack.com/jonathan-dean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>TIFFANY FUMIKO TAY</td>\n",
       "      <td>10</td>\n",
       "      <td>https://muckrack.com/tiffany-fumiko-tay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>Scott McDermott</td>\n",
       "      <td>10</td>\n",
       "      <td>https://muckrack.com/scott-mcdermott</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3844 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author  article_count  \\\n",
       "0              Neil Shaw            523   \n",
       "1           James Rodger            512   \n",
       "2             Jack Davis            251   \n",
       "3             Adam Wells            223   \n",
       "4          Sophie McCoid            218   \n",
       "...                  ...            ...   \n",
       "3839       Meredith Cohn             10   \n",
       "3840         Kate Duguid             10   \n",
       "3841       Jonathan Dean             10   \n",
       "3842  TIFFANY FUMIKO TAY             10   \n",
       "3843     Scott McDermott             10   \n",
       "\n",
       "                                     Muckrack  \n",
       "0              https://muckrack.com/neil-shaw  \n",
       "1           https://muckrack.com/james-rodger  \n",
       "2             https://muckrack.com/jack-davis  \n",
       "3             https://muckrack.com/adam-wells  \n",
       "4          https://muckrack.com/sophie-mccoid  \n",
       "...                                       ...  \n",
       "3839       https://muckrack.com/meredith-cohn  \n",
       "3840         https://muckrack.com/kate-duguid  \n",
       "3841       https://muckrack.com/jonathan-dean  \n",
       "3842  https://muckrack.com/tiffany-fumiko-tay  \n",
       "3843     https://muckrack.com/scott-mcdermott  \n",
       "\n",
       "[3844 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    info = {}\n",
    "    info['name'] = data.iloc[j].author\n",
    "    if soup.find_all(class_ = 'profile-verified'):\n",
    "        info['verified'] = True\n",
    "    if soup.find(class_ = 'person-details-item person-details-beats'):\n",
    "        info['interests'] = ' '.join([line.strip() for line in soup.find(class_ = 'fa fa-fw fa-tasks icon-standard').findNext('div').text.split('\\n') if line.strip() != ''])\n",
    "    if soup.find(class_ = 'person-details-item person-details-title'):\n",
    "        info['title'] = ' '.join([line.strip() for line in soup.find(class_ = 'fa fa-fw fa-building icon-standard').findNext('div').text.split('\\n') if line.strip() != ''])\n",
    "    if soup.find(class_ = 'person-details-item person-details-location'):\n",
    "        info['location'] = ' '.join([line.strip() for line in soup.find(class_ = 'fa fa-fw fa-map-marker icon-standard').findNext('div').text.split('\\n') if line.strip() != ''])\n",
    "    if soup.find(class_ = 'mr-font-size-lg mr-font-family-2 top-sm'):\n",
    "        info['description'] = soup.find(class_ = 'mr-font-size-lg mr-font-family-2 top-sm').text.strip()\n",
    "    info['seen_in'] = list(set([i.text for i in soup.find_all('a') if '/media-outlet/' in i.attrs['href']]))\n",
    "    if soup.find(class_ = 'profile-section-social'):\n",
    "        for i in soup.find(class_ = 'profile-section-social'):\n",
    "            temp = [j[5:] for j in str(i).split() if 'href' in j]\n",
    "            if temp: \n",
    "                temp = temp[0].strip('\"')\n",
    "                if 't.co' in temp:\n",
    "                    info['website'] = temp\n",
    "                if 'twitter' in temp:\n",
    "                    info['twitter'] = temp\n",
    "                if 'linkedin' in temp:\n",
    "                    info['linkedin'] = temp\n",
    "                if 'facebook' in temp:\n",
    "                    info['facebook'] = temp\n",
    "                if 'instagram' in temp:\n",
    "                    info['instagram'] = temp      \n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Neil Shaw', 'verified': True, 'interests': 'United Kingdom', 'title': 'Network Content Editor â€” DevonLive, Reach plc', 'location': 'England', 'description': 'Network Content Editor for Reach Plc. All of my words are second-hand, and useless in the face of this.', 'seen_in': ['The Washington Post', 'Reach plc', 'Al Jazeera', 'Daily Star (UK)', 'MSN South Africa', 'Fox News', 'Associated Press', 'DevonLive', 'Daily Mirror', 'The (Toronto) Star', 'MSN UK', 'Sunday Mirror'], 'website': 'https://t.co/dTqiiMyFdj', 'twitter': 'http://twitter.com/neiljshaw', 'linkedin': 'https://www.linkedin.com/in/neil-shaw-b45369a/'}\n",
      "\n",
      "{'name': 'James Rodger', 'verified': True, 'interests': 'United Kingdom', 'title': 'Head of Trends â€” Birmingham Live', 'location': 'Birmingham', 'description': 'Regional Content Editor for @birmingham_live.', 'seen_in': ['Wales on Sunday', 'Rochdale Observer', 'Daily Star (UK)', 'Western Mail', 'MSN South Africa', 'Daily Mirror', 'Birmingham Live', 'MSN UK', 'Sunday Mirror', 'Liverpool Echo', 'Sunday Mercury'], 'twitter': 'http://twitter.com/jamesdrodger', 'facebook': 'https://www.facebook.com/jamesrodgerjournalist', 'linkedin': 'https://www.linkedin.com/in/james-rodger-4325348a/'}\n",
      "\n",
      "{'name': 'Jack Davis', 'verified': True, 'interests': 'Politics, Religion', 'title': 'Freelance Writer â€” Freelance', 'location': 'New York', 'seen_in': ['Bozeman Daily Chronicle', 'The Western Journal', 'Freelance', 'Farms.com', 'Entrepreneur Magazine', 'Salt Lake Tribune', 'Bristol Post', 'AudioFile Magazine', 'Irish Times', 'News Medical', 'WND']}\n",
      "\n",
      "{'name': 'Adam Wells', 'verified': True, 'interests': 'Sports, U.S.', 'title': 'Writer â€” Bleacher Report', 'location': 'Boston', 'description': 'Kinda funny looking, just in a general sorta way. I am in Sports.', 'seen_in': ['Yahoo', 'Bleacher Report', 'JD Supra', 'True Median', 'Saints Report', 'NBA.com'], 'website': 'https://bleacherreport.com/users/340764', 'twitter': 'http://twitter.com/adamwells1985', 'linkedin': 'https://www.linkedin.com/in/adam-wells-07a756ab/'}\n",
      "\n",
      "{'name': 'Sophie McCoid', 'verified': True, 'interests': 'Arts and Entertainment, United Kingdom', 'title': 'Senior Reporter â€” Liverpool Echo', 'location': 'Liverpool', 'description': 'Senior Reporter for @LivEchonews, Covering TV, Showbiz and everything in between Got a story? sophie.mccoid@reachplc.com', 'seen_in': ['Wales on Sunday', 'Rochdale Observer', 'Daily Star (UK)', 'Western Mail', 'MSN South Africa', 'Daily Mirror', 'Birmingham Live', 'MSN UK', 'Sunday Mirror', 'Liverpool Echo', 'Sunday Mercury'], 'twitter': 'http://twitter.com/MccoidSophie'}\n",
      "\n",
      "{'name': 'Joseph Zucker', 'verified': True, 'interests': 'Sports, U.S.', 'title': 'Breaking News Writer â€” Bleacher Report', 'description': \"@bleacherreport breaking news writer. Ohio's biggest Seattle Storm fan\", 'seen_in': ['High Post Hoops', 'FanSided', 'RealClearLife', 'Bleacher Report', 'The Oklahoma Eagle', 'KZDC-AM (San Antonio, TX)', 'The Independent', 'The Charlotte Observer', 'Saints Report'], 'website': 'https://t.co/jGmeIFUX06', 'twitter': 'http://twitter.com/JosephZucker', 'linkedin': 'https://www.linkedin.com/in/joseph-zucker-49956693'}\n",
      "\n",
      "{'name': 'Jack Otway', 'verified': True, 'interests': 'Sports, United Kingdom', 'title': 'Freelance Journalist, Reporter â€” Daily Express, Screen Rant', 'location': 'England', 'description': 'Senior sport reporter for the Daily Express Online âš½\\nFreelance journalist for Screen Rant ðŸŽ¥\\nPrevious work: Daily Mail, GiveMeSport, WhatCulture, Daily Star', 'seen_in': ['Daily Star (UK)', 'Bleacher Report', 'WhatCulture', 'Screen Rant', 'Daily Express'], 'twitter': 'http://twitter.com/JackOtwayJourno', 'linkedin': 'https://www.linkedin.com/in/jack-otway-2422a0103', 'instagram': 'https://www.instagram.com/jack_otway123/?hl=en'}\n",
      "\n",
      "{'name': 'Simon Duke', 'verified': True, 'interests': 'Media, Technology, United Kingdom', 'title': 'Journalist â€” The Times', 'location': 'London', 'description': 'News, comment and features on technology and media for the Times. @TimesBusiness', 'seen_in': ['The Sunday Times', 'Wales on Sunday', 'Rochdale Observer', 'Western Mail', 'The Times', 'Daily Mirror', 'The Australian', 'Manchester Evening News', 'Birmingham Live', 'Sunday Mirror', 'Liverpool Echo'], 'website': 'https://t.co/GAsNZjVp0D', 'twitter': 'http://twitter.com/SimonDukeTimes', 'linkedin': 'https://www.linkedin.com/in/simon-duke-161245170/'}\n",
      "\n",
      "{'name': 'Maren Estrada', 'verified': True, 'interests': 'Technology', 'title': 'Contributor â€” BGR', 'seen_in': ['Yahoo', 'MSN Canada', 'Yahoo Tech', 'Decider', 'Epoch Times', 'MSN', 'Yahoo Singapore', 'BGR']}\n",
      "\n",
      "{'name': 'Dave Johnson', 'verified': True, 'interests': 'Canada', 'title': 'Reporter and Photographer â€” Welland Tribune', 'location': 'Lake Erie, Niagara', 'description': 'Reporter/photographer at The Welland Tribune (Torstar/Metroland Media)', 'seen_in': ['Yahoo', 'Common Dreams', 'Heavy.com', 'KHOU-TV (Houston, TX)', 'Salon', 'Medium', 'Out Magazine', 'The (Toronto) Star', 'Welland Tribune', 'Business Insider Singapore', 'Toronto Sun'], 'twitter': 'http://twitter.com/DaveJTheTrib', 'instagram': 'https://www.instagram.com/davejthetrib/?hl=en'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(data)):\n",
    "    try:\n",
    "        url = data.iloc[j].Muckrack\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        sauce = urlopen(req).read()\n",
    "    except:\n",
    "        print(\"wrong url\", j, data.iloc[j].author)\n",
    "        query = {\"query\": f\"{data.iloc[j].author} site: muckrack\"}\n",
    "        url = algo.pipe(query).result\n",
    "        url = url[0]['url']\n",
    "        if 'muckrack' in url:    \n",
    "            print(\"right url\", url)\n",
    "            print()\n",
    "            req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            sauce = urlopen(req).read()\n",
    "    soup = bs.BeautifulSoup(sauce,'lxml')\n",
    "    extract_data(soup)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
